[
  {
    "question": "What is the main architectural innovation introduced in the Transformer model?",
    "ground_truth": "The main innovation is the Transformer architecture that relies entirely on self-attention mechanisms to compute representations of input and output, dispensing with recurrence and convolutions entirely."
  },
  {
    "question": "How many layers are used in the base Transformer model for both encoder and decoder?",
    "ground_truth": "The base model uses N=6 identical layers for both the encoder and decoder stacks."
  },
  {
    "question": "What is the model dimension (d_model) in the base Transformer?",
    "ground_truth": "The base Transformer uses d_model=512, which is the dimension of the model throughout all layers."
  },
  {
    "question": "How many attention heads does the base Transformer model use?",
    "ground_truth": "The base model uses h=8 parallel attention heads in the multi-head attention mechanism."
  },
  {
    "question": "What is the dimension of the feedforward network (d_ff) in the base Transformer?",
    "ground_truth": "The feedforward network has dimension d_ff=2048, which is the inner-layer dimensionality."
  },
  {
    "question": "What activation function is used in the position-wise feedforward networks?",
    "ground_truth": "The feedforward networks use ReLU activation function between the two linear transformations."
  },
  {
    "question": "What dropout rate is used during training of the base model?",
    "ground_truth": "A dropout rate of P_drop=0.1 is applied to the output of each sub-layer, before it is added to the sub-layer input and normalized."
  },
  {
    "question": "What type of positional encoding does the Transformer use?",
    "ground_truth": "The Transformer uses sinusoidal positional encodings based on sine and cosine functions of different frequencies to inject information about the relative or absolute position of tokens."
  },
  {
    "question": "What optimizer is used to train the Transformer model?",
    "ground_truth": "The Adam optimizer is used with β1=0.9, β2=0.98, and ε=10^-9."
  },
  {
    "question": "How does the learning rate vary during training?",
    "ground_truth": "The learning rate increases linearly for the first warmup_steps training steps, and then decreases proportionally to the inverse square root of the step number."
  },
  {
    "question": "How many warmup steps are used in the learning rate schedule?",
    "ground_truth": "The learning rate schedule uses warmup_steps=4000."
  },
  {
    "question": "What regularization technique besides dropout is mentioned in the paper?",
    "ground_truth": "Label smoothing of value ε_ls=0.1 is used during training, which hurts perplexity but improves accuracy and BLEU score."
  },
  {
    "question": "What are the three types of attention mechanisms used in the Transformer?",
    "ground_truth": "The three types are: encoder-decoder attention (decoder attends to encoder output), encoder self-attention (encoder attends to previous encoder layer), and masked decoder self-attention (decoder attends to previous decoder positions)."
  },
  {
    "question": "What BLEU score did the base Transformer achieve on WMT 2014 English-to-German translation?",
    "ground_truth": "The base model achieved a BLEU score of 27.3 on the WMT 2014 English-to-German translation task."
  },
  {
    "question": "What BLEU score did the big Transformer model achieve on WMT 2014 English-to-French translation?",
    "ground_truth": "The big model achieved a BLEU score of 41.0 on the WMT 2014 English-to-French translation task, setting a new state-of-the-art."
  },
  {
    "question": "How many training steps were used for the base model?",
    "ground_truth": "The base models were trained for 100,000 steps or 12 hours."
  },
  {
    "question": "What hardware was used to train the base Transformer models?",
    "ground_truth": "The base models were trained on one machine with 8 NVIDIA P100 GPUs."
  },
  {
    "question": "How long did it take to train the big Transformer model?",
    "ground_truth": "The big models were trained for 300,000 steps (3.5 days) on 8 P100 GPUs."
  },
  {
    "question": "What is the computational complexity of self-attention per layer?",
    "ground_truth": "Self-attention has complexity O(n^2·d) per layer, where n is the sequence length and d is the representation dimension."
  },
  {
    "question": "What is the key advantage of self-attention over recurrent layers in terms of parallelization?",
    "ground_truth": "Self-attention requires O(1) sequential operations compared to O(n) for recurrent layers, making it highly parallelizable and faster to train."
  }
]
