[
  {
    "question": "What is the main architectural innovation introduced in the Transformer model?",
    "ground_truth": "The main innovation is the self-attention mechanism that replaces recurrence and convolutions entirely, allowing the model to process sequences in parallel while capturing long-range dependencies through attention."
  },
  {
    "question": "How many layers are used in the base Transformer model for both encoder and decoder?",
    "ground_truth": "The base Transformer model uses 6 layers (N=6) for both the encoder and decoder stacks."
  },
  {
    "question": "What is multi-head attention and how many heads does the base model use?",
    "ground_truth": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. The base model uses 8 parallel attention heads (h=8)."
  },
  {
    "question": "What are the three types of attention mechanisms used in the Transformer architecture?",
    "ground_truth": "The three types are: (1) Encoder self-attention where encoder layers attend to all positions in the previous encoder layer, (2) Decoder self-attention where decoder layers attend to previous positions in the decoder, and (3) Encoder-decoder attention where decoder attends to encoder outputs."
  },
  {
    "question": "What is the model dimension (d_model) and feedforward dimension (d_ff) in the base Transformer?",
    "ground_truth": "The base model uses d_model=512 for the model dimension and d_ff=2048 for the inner dimension of the feedforward networks."
  },
  {
    "question": "What optimizer and learning rate schedule does the Transformer use during training?",
    "ground_truth": "The Transformer uses the Adam optimizer with β1=0.9, β2=0.98, and ε=10^-9. The learning rate increases linearly for the first warmup_steps=4000 training steps, then decreases proportionally to the inverse square root of the step number."
  },
  {
    "question": "What BLEU scores did the Transformer achieve on WMT 2014 English-to-German and English-to-French translation tasks?",
    "ground_truth": "On WMT 2014 English-to-German translation, the big Transformer model achieved 28.4 BLEU, establishing a new state-of-the-art. On English-to-French translation, it achieved 41.8 BLEU."
  },
  {
    "question": "Why does the Transformer use positional encodings, and what approach is used?",
    "ground_truth": "Since the Transformer contains no recurrence or convolution, positional encodings are added to give the model information about the relative or absolute position of tokens in the sequence. The paper uses sine and cosine functions of different frequencies to encode positions."
  },
  {
    "question": "What is the computational complexity advantage of self-attention compared to recurrent layers?",
    "ground_truth": "Self-attention layers have O(1) sequential operations regardless of sequence length, compared to O(n) for recurrent layers. The complexity per layer is O(n^2·d) for self-attention vs O(n·d^2) for recurrent layers, making self-attention faster when sequence length n is smaller than representation dimension d."
  },
  {
    "question": "What regularization techniques are used in the Transformer during training?",
    "ground_truth": "The Transformer uses three types of regularization: (1) Residual dropout with rate 0.1 applied to the output of each sub-layer before addition and normalization, (2) Dropout on attention weights, and (3) Label smoothing with value 0.1 which hurts perplexity but improves accuracy and BLEU."
  },
  {
    "question": "How much training time and hardware did the base and big Transformer models require?",
    "ground_truth": "The base models were trained for 100,000 steps or 12 hours on 8 NVIDIA P100 GPUs. The big models were trained for 300,000 steps or 3.5 days on the same hardware."
  },
  {
    "question": "What are the key advantages of the Transformer architecture over previous sequence-to-sequence models?",
    "ground_truth": "The key advantages are: (1) Enables parallelization during training since it doesn't require sequential processing like RNNs, (2) Reduces path length for learning long-range dependencies to O(1) operations, (3) Achieves better translation quality with lower training costs, and (4) Shows strong performance on other tasks like English constituency parsing."
  }
]
